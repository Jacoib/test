GBDT

梯度提升决策树（Gradient Boosting Seceision Tree,GBDT)是boosting算法中非常流行的模型，体现了从错误中学习的理念。从错误中学习，也是boosting类方法最明显的特点。

首先，我们来说一下什么是boosting方法。

boosting方法的第一个特点是个体学习器见存在强依赖关系，必须串行生产的序列化方法。

boosting方法的基本过程是，先从初始训练集训练处一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如果重复进行，直至基学习数目达到实现指定的值T ,最终将这T个基学习器进行加权结合。这就是从错误中学习的理念。

boosting能够提升弱分类器性能的原因是降低了偏差。

在boosting的每一轮迭代中，基于已生成的弱分类器集合（即当前模型）的预测及诶过，新的弱分类器会重点关注那些还没有被正确预测的样本。

Gradient Boosting是boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。

在每一轮的迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。

GBDT的优点和局限性有哪些？

优点

预测节点的计算速度快，树与树之间可并行计算
在分布稠密的数据集上，泛化能力和表达能力都很好
采用决策树作为弱分类器使得GBDT模型具有很好的解释性和鲁棒性，能够发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理如归一化等
局限性

GBDT在处理高维稀疏的手机谁给你，表现不如支持向量机或者神经网络
GBDT在处理文本分类特征上，相对其他模型的优势不如它在处理数值特征时明显
训练过程需要串行训练，只能在决策树内采用一些局部并行的手段提升训练速度。
